{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc5ff23",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import time\n",
    "import random\n",
    "\n",
    "try:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Complex Spark Job\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    print(\"Starting complex Spark job...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Generate a larger dataset (1 million records)\n",
    "    print(\"Step 1: Generating large dataset...\")\n",
    "    large_data = []\n",
    "    for i in range(1000000):\n",
    "        large_data.append((\n",
    "            f\"User_{i % 10000}\",  # 10k unique users\n",
    "            random.randint(18, 80),  # Age\n",
    "            random.choice(['M', 'F']),  # Gender\n",
    "            random.choice(['NY', 'CA', 'TX', 'FL', 'WA']),  # State\n",
    "            random.uniform(1000, 10000),  # Salary\n",
    "            random.choice(['Tech', 'Finance', 'Healthcare', 'Education', 'Retail'])  # Industry\n",
    "        ))\n",
    "    \n",
    "    df = spark.createDataFrame(large_data, [\"UserID\", \"Age\", \"Gender\", \"State\", \"Salary\", \"Industry\"])\n",
    "    df.cache()  # Cache for multiple operations\n",
    "    \n",
    "    print(f\"Created dataset with {df.count():,} records\")\n",
    "\n",
    "    # Complex transformations and aggregations\n",
    "    print(\"Step 2: Performing complex aggregations...\")\n",
    "    \n",
    "    # Age group analysis\n",
    "    df_with_age_groups = df.withColumn(\n",
    "        \"AgeGroup\",\n",
    "        when(col(\"Age\") < 25, \"18-24\")\n",
    "        .when(col(\"Age\") < 35, \"25-34\")\n",
    "        .when(col(\"Age\") < 45, \"35-44\")\n",
    "        .when(col(\"Age\") < 55, \"45-54\")\n",
    "        .otherwise(\"55+\")\n",
    "    )\n",
    "    \n",
    "    # Multiple complex aggregations\n",
    "    age_stats = df_with_age_groups.groupBy(\"AgeGroup\", \"State\", \"Industry\") \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"Count\"),\n",
    "            avg(\"Salary\").alias(\"AvgSalary\"),\n",
    "            stddev(\"Salary\").alias(\"SalaryStdDev\"),\n",
    "            min(\"Salary\").alias(\"MinSalary\"),\n",
    "            max(\"Salary\").alias(\"MaxSalary\")\n",
    "        ) \\\n",
    "        .orderBy(\"AgeGroup\", \"State\", \"Industry\")\n",
    "    \n",
    "    print(\"Age group statistics by state and industry:\")\n",
    "    age_stats.show(50)\n",
    "\n",
    "    print(\"Step 3: Complex joins and window functions...\")\n",
    "    \n",
    "    # Create a second dataset for joining\n",
    "    salary_benchmarks = spark.createDataFrame([\n",
    "        (\"Tech\", 8000), (\"Finance\", 7500), (\"Healthcare\", 6500),\n",
    "        (\"Education\", 5000), (\"Retail\", 4500)\n",
    "    ], [\"Industry\", \"BenchmarkSalary\"])\n",
    "    \n",
    "    # Join with salary benchmarks\n",
    "    df_with_benchmark = df.join(salary_benchmarks, \"Industry\")\n",
    "    \n",
    "    # Window functions for ranking\n",
    "    from pyspark.sql.window import Window\n",
    "    \n",
    "    window_spec = Window.partitionBy(\"State\", \"Industry\").orderBy(desc(\"Salary\"))\n",
    "    \n",
    "    df_ranked = df_with_benchmark.withColumn(\n",
    "        \"SalaryRank\", \n",
    "        row_number().over(window_spec)\n",
    "    ).withColumn(\n",
    "        \"SalaryPercentile\",\n",
    "        percent_rank().over(window_spec)\n",
    "    ).withColumn(\n",
    "        \"SalaryVsBenchmark\",\n",
    "        round((col(\"Salary\") / col(\"BenchmarkSalary\") - 1) * 100, 2)\n",
    "    )\n",
    "    \n",
    "    # Get top performers by state\n",
    "    top_performers = df_ranked.filter(col(\"SalaryRank\") <= 10)\n",
    "    print(\"Top 10 salary performers by state and industry:\")\n",
    "    top_performers.select(\"State\", \"Industry\", \"UserID\", \"Salary\", \"SalaryVsBenchmark\").show(100)\n",
    "\n",
    "    print(\"Step 4: Complex statistical operations...\")\n",
    "    \n",
    "    # Correlation analysis (computationally expensive)\n",
    "    df_numeric = df.select(\"Age\", \"Salary\")\n",
    "    correlation = df_numeric.stat.corr(\"Age\", \"Salary\")\n",
    "    print(f\"Age-Salary Correlation: {correlation:.4f}\")\n",
    "    \n",
    "    # Cross-tabulation\n",
    "    crosstab = df.stat.crosstab(\"Gender\", \"Industry\")\n",
    "    print(\"Gender vs Industry cross-tabulation:\")\n",
    "    crosstab.show()\n",
    "\n",
    "    print(\"Step 5: Multiple data transformations...\")\n",
    "    \n",
    "    # Create multiple derived columns\n",
    "    df_enriched = df.withColumn(\"SalaryTier\", \n",
    "        when(col(\"Salary\") < 3000, \"Low\")\n",
    "        .when(col(\"Salary\") < 7000, \"Medium\")\n",
    "        .otherwise(\"High\")\n",
    "    ).withColumn(\"IsHighEarner\", col(\"Salary\") > 8000) \\\n",
    "    .withColumn(\"NormalizedAge\", (col(\"Age\") - 18) / (80 - 18)) \\\n",
    "    .withColumn(\"SalaryPerAge\", col(\"Salary\") / col(\"Age\"))\n",
    "    \n",
    "    # Final complex aggregation\n",
    "    final_summary = df_enriched.groupBy(\"State\") \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"TotalUsers\"),\n",
    "            countDistinct(\"UserID\").alias(\"UniqueUsers\"),\n",
    "            avg(\"Salary\").alias(\"AvgSalary\"),\n",
    "            sum(when(col(\"IsHighEarner\"), 1).otherwise(0)).alias(\"HighEarners\"),\n",
    "            collect_list(\"Industry\").alias(\"Industries\")\n",
    "        )\n",
    "    \n",
    "    print(\"Final state summary:\")\n",
    "    final_summary.show(truncate=False)\n",
    "\n",
    "    # Force evaluation with an action that processes all data\n",
    "    print(\"Step 6: Final data processing...\")\n",
    "    total_records = df_enriched.count()\n",
    "    high_earner_percentage = df_enriched.filter(col(\"IsHighEarner\")).count() / total_records * 100\n",
    "    \n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    \n",
    "    print(f\"\\nJob completed!\")\n",
    "    print(f\"Total records processed: {total_records:,}\")\n",
    "    print(f\"High earner percentage: {high_earner_percentage:.2f}%\")\n",
    "    print(f\"Processing time: {processing_time:.2f} seconds\")\n",
    "    \n",
    "    # Clean up\n",
    "    df.unpersist()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    time.sleep(5)  # Brief pause before cleanup\n",
    "    # Stop the Spark session\n",
    "    if 'spark' in locals():\n",
    "        spark.stop()\n",
    "        print(\"Spark session stopped\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
