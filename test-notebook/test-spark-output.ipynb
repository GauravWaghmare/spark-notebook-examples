{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bb7be1a",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [3]</a>'.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9be8bbaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T06:54:07.359391Z",
     "iopub.status.busy": "2026-01-07T06:54:07.358978Z",
     "iopub.status.idle": "2026-01-07T06:54:07.368265Z",
     "shell.execute_reply": "2026-01-07T06:54:07.367442Z"
    },
    "papermill": {
     "duration": 0.01758,
     "end_time": "2026-01-07T06:54:07.369266",
     "exception": false,
     "start_time": "2026-01-07T06:54:07.351686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/gauravwaghmare/Documents/Personal/spark-notebook-examples\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6290ad6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T06:54:07.372975Z",
     "iopub.status.busy": "2026-01-07T06:54:07.372815Z",
     "iopub.status.idle": "2026-01-07T06:54:07.376432Z",
     "shell.execute_reply": "2026-01-07T06:54:07.375619Z"
    },
    "papermill": {
     "duration": 0.006011,
     "end_time": "2026-01-07T06:54:07.377102",
     "exception": false,
     "start_time": "2026-01-07T06:54:07.371091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World!\n",
      "Hello, World!\n"
     ]
    }
   ],
   "source": [
    "from hello.hello import hello_world\n",
    "\n",
    "hello_world()\n",
    "hello_world()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31b49e0",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcc5ff23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T06:54:07.380581Z",
     "iopub.status.busy": "2026-01-07T06:54:07.380427Z",
     "iopub.status.idle": "2026-01-07T06:54:07.478540Z",
     "shell.execute_reply": "2026-01-07T06:54:07.478190Z"
    },
    "papermill": {
     "duration": 0.100763,
     "end_time": "2026-01-07T06:54:07.479329",
     "exception": true,
     "start_time": "2026-01-07T06:54:07.378566",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import time\n",
    "import random\n",
    "\n",
    "try:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Complex Spark Job\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    print(\"Starting complex Spark job...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Generate a larger dataset (1 million records)\n",
    "    print(\"Step 1: Generating large dataset...\")\n",
    "    large_data = []\n",
    "    for i in range(1000000):\n",
    "        large_data.append((\n",
    "            f\"User_{i % 10000}\",  # 10k unique users\n",
    "            random.randint(18, 80),  # Age\n",
    "            random.choice(['M', 'F']),  # Gender\n",
    "            random.choice(['NY', 'CA', 'TX', 'FL', 'WA']),  # State\n",
    "            random.uniform(1000, 10000),  # Salary\n",
    "            random.choice(['Tech', 'Finance', 'Healthcare', 'Education', 'Retail'])  # Industry\n",
    "        ))\n",
    "    \n",
    "    df = spark.createDataFrame(large_data, [\"UserID\", \"Age\", \"Gender\", \"State\", \"Salary\", \"Industry\"])\n",
    "    df.cache()  # Cache for multiple operations\n",
    "    \n",
    "    print(f\"Created dataset with {df.count():,} records\")\n",
    "\n",
    "    # Complex transformations and aggregations\n",
    "    print(\"Step 2: Performing complex aggregations...\")\n",
    "    \n",
    "    # Age group analysis\n",
    "    df_with_age_groups = df.withColumn(\n",
    "        \"AgeGroup\",\n",
    "        when(col(\"Age\") < 25, \"18-24\")\n",
    "        .when(col(\"Age\") < 35, \"25-34\")\n",
    "        .when(col(\"Age\") < 45, \"35-44\")\n",
    "        .when(col(\"Age\") < 55, \"45-54\")\n",
    "        .otherwise(\"55+\")\n",
    "    )\n",
    "    \n",
    "    # Multiple complex aggregations\n",
    "    age_stats = df_with_age_groups.groupBy(\"AgeGroup\", \"State\", \"Industry\") \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"Count\"),\n",
    "            avg(\"Salary\").alias(\"AvgSalary\"),\n",
    "            stddev(\"Salary\").alias(\"SalaryStdDev\"),\n",
    "            min(\"Salary\").alias(\"MinSalary\"),\n",
    "            max(\"Salary\").alias(\"MaxSalary\")\n",
    "        ) \\\n",
    "        .orderBy(\"AgeGroup\", \"State\", \"Industry\")\n",
    "    \n",
    "    print(\"Age group statistics by state and industry:\")\n",
    "    age_stats.show(50)\n",
    "\n",
    "    print(\"Step 3: Complex joins and window functions...\")\n",
    "    \n",
    "    # Create a second dataset for joining\n",
    "    salary_benchmarks = spark.createDataFrame([\n",
    "        (\"Tech\", 8000), (\"Finance\", 7500), (\"Healthcare\", 6500),\n",
    "        (\"Education\", 5000), (\"Retail\", 4500)\n",
    "    ], [\"Industry\", \"BenchmarkSalary\"])\n",
    "    \n",
    "    # Join with salary benchmarks\n",
    "    df_with_benchmark = df.join(salary_benchmarks, \"Industry\")\n",
    "    \n",
    "    # Window functions for ranking\n",
    "    from pyspark.sql.window import Window\n",
    "    \n",
    "    window_spec = Window.partitionBy(\"State\", \"Industry\").orderBy(desc(\"Salary\"))\n",
    "    \n",
    "    df_ranked = df_with_benchmark.withColumn(\n",
    "        \"SalaryRank\", \n",
    "        row_number().over(window_spec)\n",
    "    ).withColumn(\n",
    "        \"SalaryPercentile\",\n",
    "        percent_rank().over(window_spec)\n",
    "    ).withColumn(\n",
    "        \"SalaryVsBenchmark\",\n",
    "        round((col(\"Salary\") / col(\"BenchmarkSalary\") - 1) * 100, 2)\n",
    "    )\n",
    "    \n",
    "    # Get top performers by state\n",
    "    top_performers = df_ranked.filter(col(\"SalaryRank\") <= 10)\n",
    "    print(\"Top 10 salary performers by state and industry:\")\n",
    "    top_performers.select(\"State\", \"Industry\", \"UserID\", \"Salary\", \"SalaryVsBenchmark\").show(100)\n",
    "\n",
    "    print(\"Step 4: Complex statistical operations...\")\n",
    "    \n",
    "    # Correlation analysis (computationally expensive)\n",
    "    df_numeric = df.select(\"Age\", \"Salary\")\n",
    "    correlation = df_numeric.stat.corr(\"Age\", \"Salary\")\n",
    "    print(f\"Age-Salary Correlation: {correlation:.4f}\")\n",
    "    \n",
    "    # Cross-tabulation\n",
    "    crosstab = df.stat.crosstab(\"Gender\", \"Industry\")\n",
    "    print(\"Gender vs Industry cross-tabulation:\")\n",
    "    crosstab.show()\n",
    "\n",
    "    print(\"Step 5: Multiple data transformations...\")\n",
    "    \n",
    "    # Create multiple derived columns\n",
    "    df_enriched = df.withColumn(\"SalaryTier\", \n",
    "        when(col(\"Salary\") < 3000, \"Low\")\n",
    "        .when(col(\"Salary\") < 7000, \"Medium\")\n",
    "        .otherwise(\"High\")\n",
    "    ).withColumn(\"IsHighEarner\", col(\"Salary\") > 8000) \\\n",
    "    .withColumn(\"NormalizedAge\", (col(\"Age\") - 18) / (80 - 18)) \\\n",
    "    .withColumn(\"SalaryPerAge\", col(\"Salary\") / col(\"Age\"))\n",
    "    \n",
    "    # Final complex aggregation\n",
    "    final_summary = df_enriched.groupBy(\"State\") \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"TotalUsers\"),\n",
    "            countDistinct(\"UserID\").alias(\"UniqueUsers\"),\n",
    "            avg(\"Salary\").alias(\"AvgSalary\"),\n",
    "            sum(when(col(\"IsHighEarner\"), 1).otherwise(0)).alias(\"HighEarners\"),\n",
    "            collect_list(\"Industry\").alias(\"Industries\")\n",
    "        )\n",
    "    \n",
    "    print(\"Final state summary:\")\n",
    "    final_summary.show(truncate=False)\n",
    "\n",
    "    # Force evaluation with an action that processes all data\n",
    "    print(\"Step 6: Final data processing...\")\n",
    "    total_records = df_enriched.count()\n",
    "    high_earner_percentage = df_enriched.filter(col(\"IsHighEarner\")).count() / total_records * 100\n",
    "    \n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    \n",
    "    print(f\"\\nJob completed!\")\n",
    "    print(f\"Total records processed: {total_records:,}\")\n",
    "    print(f\"High earner percentage: {high_earner_percentage:.2f}%\")\n",
    "    print(f\"Processing time: {processing_time:.2f} seconds\")\n",
    "    \n",
    "    # Clean up\n",
    "    df.unpersist()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    time.sleep(5)  # Brief pause before cleanup\n",
    "    # Stop the Spark session\n",
    "    if 'spark' in locals():\n",
    "        spark.stop()\n",
    "        print(\"Spark session stopped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 0.814037,
   "end_time": "2026-01-07T06:54:07.594457",
   "environment_variables": {},
   "exception": true,
   "input_path": "/Users/gauravwaghmare/Documents/Personal/spark-notebook-examples/test-notebook/test-spark.ipynb",
   "output_path": "/Users/gauravwaghmare/Documents/Personal/spark-notebook-examples/test-notebook/test-spark-output.ipynb",
   "parameters": {},
   "start_time": "2026-01-07T06:54:06.780420",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}